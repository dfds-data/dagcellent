"""
This type stub file was generated by pyright.
"""

import asyncio
import threading
from collections import deque
from typing import TYPE_CHECKING

from airflow.jobs.base_job_runner import BaseJobRunner
from airflow.jobs.job import Job
from airflow.models import TaskInstance
from airflow.traces.tracer import span
from airflow.triggers.base import BaseTrigger, TriggerEvent
from airflow.typing_compat import TypedDict
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.utils.session import provide_session
from sqlalchemy.orm import Session

if TYPE_CHECKING: ...
HANDLER_SUPPORTS_TRIGGERER = ...
SEND_TRIGGER_END_MARKER = ...
logger = ...
DISABLE_WRAPPER = ...
DISABLE_LISTENER = ...

def configure_trigger_log_handler():  # -> None:
    """
    Configure logging where each trigger logs to its own file and can be exposed via the airflow webserver.

    Generally speaking, we take the log handler configured for logger ``airflow.task``,
    wrap it with TriggerHandlerWrapper, and set it as the handler for root logger.

    If there already is a handler configured for the root logger and it supports triggers, we wrap it instead.

    :meta private:
    """
    ...

def setup_queue_listener():  # -> None:
    """
    Route log messages to a queue and process them with QueueListener.

    Airflow task handlers make blocking I/O calls.
    We replace trigger log handlers, with LocalQueueHandler,
    which sends log records to a queue.
    Then we start a QueueListener in a thread, which is configured
    to consume the queue and pass the records to the handlers as
    originally configured. This keeps the handler I/O out of the
    async event loop.

    :meta private:
    """
    ...

class TriggererJobRunner(BaseJobRunner, LoggingMixin):
    """
    Run active triggers in asyncio and update their dependent tests/DAGs once their events have fired.

    It runs as two threads:
     - The main thread does DB calls/checkins
     - A subthread runs all the async code
    """

    job_type = ...
    def __init__(self, job: Job, capacity=...) -> None: ...
    @provide_session
    def heartbeat_callback(self, session: Session = ...) -> None: ...
    def register_signals(self) -> None:
        """Register signals that stop child processes."""
        ...

    @classmethod
    @provide_session
    def is_needed(cls, session) -> bool:
        """
        Test if the triggerer job needs to be run (i.e., if there are triggers in the trigger table).

        This is used for the warning boxes in the UI.
        """
        ...

    def on_kill(self):  # -> None:
        """
        Stop the trigger runner.

        Called when there is an external kill command (via the heartbeat mechanism, for example).
        """
        ...

    @span
    def load_triggers(self):  # -> None:
        """Query the database for the triggers we're supposed to be running and update the runner."""
        ...

    @span
    def handle_events(self):  # -> None:
        """Dispatch outbound events to the Trigger model which pushes them to the relevant task instances."""
        ...

    @span
    def handle_failed_triggers(self):  # -> None:
        """
        Handle "failed" triggers. - ones that errored or exited before they sent an event.

        Task Instances that depend on them need failing.
        """
        ...

    @span
    def emit_metrics(self):  # -> None:
        ...

class TriggerDetails(TypedDict):
    """Type class for the trigger details dictionary."""

    task: asyncio.Task
    name: str
    events: int
    ...

class TriggerRunner(threading.Thread, LoggingMixin):
    """
    Runtime environment for all triggers.

    Mainly runs inside its own thread, where it hands control off to an asyncio
    event loop, but is also sometimes interacted with from the main thread
    (where all the DB queries are done). All communication between threads is
    done via Deques.
    """

    triggers: dict[int, TriggerDetails]
    trigger_cache: dict[str, type[BaseTrigger]]
    to_create: deque[tuple[int, BaseTrigger]]
    to_cancel: deque[int]
    events: deque[tuple[int, TriggerEvent]]
    failed_triggers: deque[tuple[int, BaseException]]
    stop: bool = ...
    def __init__(self) -> None: ...
    def run(self):  # -> None:
        """Sync entrypoint - just run a run in an async loop."""
        ...

    async def arun(self):  # -> None:
        """
        Run trigger addition/deletion/cleanup; main (asynchronous) logic loop.

        Actual triggers run in their own separate coroutines.
        """
        ...

    async def create_triggers(self):  # -> None:
        """Drain the to_create queue and create all new triggers that have been requested in the DB."""
        ...

    async def cancel_triggers(self):  # -> None:
        """
        Drain the to_cancel queue and ensure all triggers that are not in the DB are cancelled.

        This allows the cleanup job to delete them.
        """
        ...

    async def cleanup_finished_triggers(self):  # -> None:
        """
        Go through all trigger tasks (coroutines) and clean up entries for ones that have exited.

        Optionally warn users if the exit was not normal.
        """
        ...

    async def block_watchdog(self):  # -> None:
        """
        Watchdog loop that detects blocking (badly-written) triggers.

        Triggers should be well-behaved async coroutines and await whenever
        they need to wait; this loop tries to run every 100ms to see if
        there are badly-written triggers taking longer than that and blocking
        the event loop.

        Unfortunately, we can't tell what trigger is blocking things, but
        we can at least detect the top-level problem.
        """
        ...

    @staticmethod
    def set_individual_trigger_logging(trigger):  # -> None:
        """Configure trigger logging to allow individual files and stdout filtering."""
        ...

    async def run_trigger(self, trigger_id, trigger):  # -> None:
        """Run a trigger (they are async generators) and push their events into our outbound event deque."""
        ...

    @staticmethod
    def mark_trigger_end(trigger):  # -> None:
        ...
    def update_triggers(self, requested_trigger_ids: set[int]):  # -> None:
        """
        Request that we update what triggers we're running.

        Works out the differences - ones to add, and ones to remove - then
        adds them to the deques so the subthread can actually mutate the running
        trigger set.
        """
        ...

    def set_trigger_logging_metadata(
        self, ti: TaskInstance, trigger_id, trigger
    ):  # -> None:
        """
        Set up logging for triggers.

        We want to ensure that each trigger logs to its own file and that the log messages are not
        propagated to parent loggers.

        :meta private:
        """
        ...

    def get_trigger_by_classpath(self, classpath: str) -> type[BaseTrigger]:
        """
        Get a trigger class by its classpath ("path.to.module.classname").

        Uses a cache dictionary to speed up lookups after the first time.
        """
        ...
