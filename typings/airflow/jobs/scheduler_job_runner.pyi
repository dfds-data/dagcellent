"""
This type stub file was generated by pyright.
"""

import logging
from dataclasses import dataclass
from typing import TYPE_CHECKING

from airflow.jobs.base_job_runner import BaseJobRunner
from airflow.jobs.job import Job
from airflow.models.dag import DagModel
from airflow.models.dagrun import DagRun
from airflow.models.taskinstance import TaskInstance
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.utils.session import provide_session
from sqlalchemy.orm import Session

if TYPE_CHECKING: ...
TI = TaskInstance
DR = DagRun
DM = DagModel
TASK_STUCK_IN_QUEUED_RESCHEDULE_EVENT = ...

@dataclass
class ConcurrencyMap:
    """
    Dataclass to represent concurrency maps.

    It contains a map from (dag_id, task_id) to # of task instances, a map from (dag_id, task_id)
    to # of task instances in the given state list and a map from (dag_id, run_id, task_id)
    to # of task instances in the given state list in each DAG run.
    """

    dag_active_tasks_map: dict[str, int]
    task_concurrency_map: dict[tuple[str, str], int]
    task_dagrun_concurrency_map: dict[tuple[str, str, str], int]
    @classmethod
    def from_concurrency_map(
        cls, mapping: dict[tuple[str, str, str], int]
    ) -> ConcurrencyMap: ...

class SchedulerJobRunner(BaseJobRunner, LoggingMixin):
    """
    SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.

    It figures out the latest runs for each task and sees if the dependencies
    for the next schedules are met.
    If so, it creates appropriate TaskInstances and sends run commands to the
    executor. It does this for each task in each DAG and repeats.

    :param subdir: directory containing Python files with Airflow DAG
        definitions, or a specific path to a file
    :param num_runs: The number of times to run the scheduling loop. If you
        have a large number of DAG files this could complete before each file
        has been parsed. -1 for unlimited times.
    :param num_times_parse_dags: The number of times to try to parse each DAG file.
        -1 for unlimited times.
    :param scheduler_idle_sleep_time: The number of seconds to wait between
        polls of running processors
    :param do_pickle: once a DAG object is obtained by executing the Python
        file, whether to serialize the DAG object to the DB
    :param log: override the default Logger
    """

    job_type = ...
    def __init__(
        self,
        job: Job,
        subdir: str = ...,
        num_runs: int = ...,
        num_times_parse_dags: int = ...,
        scheduler_idle_sleep_time: float = ...,
        do_pickle: bool = ...,
        log: logging.Logger | None = ...,
        processor_poll_interval: float | None = ...,
    ) -> None: ...
    @provide_session
    def heartbeat_callback(self, session: Session = ...) -> None: ...
    def register_signals(self) -> None:
        """Register signals that stop child processes."""
        ...

    @provide_session
    def adopt_or_reset_orphaned_tasks(self, session: Session = ...) -> int:
        """
        Adopt or reset any TaskInstance in resettable state if its SchedulerJob is no longer running.

        :return: the number of TIs reset
        """
        ...

    @provide_session
    def check_trigger_timeouts(
        self, max_retries: int = ..., session: Session = ...
    ) -> None:
        """Mark any "deferred" task as failed if the trigger or execution timeout has passed."""
        ...
