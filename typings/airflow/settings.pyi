"""
This type stub file was generated by pyright.
"""

import json
import traceback
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

import pluggy
from airflow.configuration import conf
from airflow.www.utils import UIAlert
from sqlalchemy.engine import Engine
from sqlalchemy.orm import Session as SASession

if TYPE_CHECKING: ...
log = ...
if tz := conf.get_mandatory_value("core", "default_timezone") != "system":
    TIMEZONE = ...
else:
    TIMEZONE = ...
if conf.has_option("database", "sql_alchemy_session_maker"): ...
HEADER = ...
LOGGING_LEVEL = ...
GUNICORN_WORKER_READY_PREFIX = ...
LOG_FORMAT = ...
SIMPLE_LOG_FORMAT = ...
SQL_ALCHEMY_CONN: str | None = ...
PLUGINS_FOLDER: str | None = ...
LOGGING_CLASS_PATH: str | None = ...
DONOT_MODIFY_HANDLERS: bool | None = ...
DAGS_FOLDER: str = ...
engine: Engine
Session: Callable[..., SASession]
json = ...
STATE_COLORS = ...

def custom_show_warning(
    message, category, filename, lineno, file=..., line=...
):  # -> None:
    """Print rich and visible warnings."""
    ...

def replace_showwarning(replacement):  # -> Callable[..., None]:
    """
    Replace ``warnings.showwarning``, returning the original.

    This is useful since we want to "reset" the ``showwarning`` hook on exit to
    avoid lazy-loading issues. If a warning is emitted after Python cleaned up
    the import system, we would no longer be able to import ``rich``.
    """
    ...

original_show_warning = ...
POLICY_PLUGIN_MANAGER: Any = ...

def task_policy(task):  # -> Any:
    ...
def dag_policy(dag):  # -> Any:
    ...
def task_instance_mutation_hook(task_instance):  # -> Any:
    ...
def pod_mutation_hook(pod):  # -> Any:
    ...
def get_airflow_context_vars(context):  # -> Any:
    ...
def get_dagbag_import_timeout(dag_file_path: str):  # -> Any:
    ...
def configure_policy_plugin_manager():  # -> None:
    ...
def load_policy_plugins(pm: pluggy.PluginManager):  # -> None:
    ...
def configure_vars():  # -> None:
    """Configure Global Variables from airflow.cfg."""
    ...

def run_providers_custom_runtime_checks():  # -> None:
    ...

class SkipDBTestsSession:
    """
    This fake session is used to skip DB tests when `_AIRFLOW_SKIP_DB_TESTS` is set.

    :meta private:
    """

    def __init__(self) -> None: ...
    def remove(*args, **kwargs):  # -> None:
        ...
    def get_bind(
        self,
        mapper=...,
        clause=...,
        bind=...,
        _sa_skip_events=...,
        _sa_skip_for_implicit_returning=...,
    ):  # -> None:
        ...

def get_cleaned_traceback(stack_summary: traceback.StackSummary) -> str: ...

class TracebackSession:
    """
    Session that throws error when you try to use it.

    Also stores stack at instantiation call site.

    :meta private:
    """

    def __init__(self) -> None: ...
    def __getattr__(self, item): ...
    def remove(*args, **kwargs):  # -> None:
        ...

AIRFLOW_PATH = ...
AIRFLOW_TESTS_PATH = ...
AIRFLOW_SETTINGS_PATH = ...
AIRFLOW_UTILS_SESSION_PATH = ...
AIRFLOW_MODELS_BASEOPERATOR_PATH = ...
AIRFLOW_MODELS_DAG_PATH = ...
AIRFLOW_DB_UTILS_PATH = ...

class TracebackSessionForTests:
    """
    Session that throws error when you try to create a session outside of the test code.

    When we run our tests in "db isolation" mode we expect that "airflow" code will never create
    a session on its own and internal_api server is used for all calls but the test code might use
    the session to setup and teardown in the DB so that the internal API server accesses it.

    :meta private:
    """

    db_session_class = ...
    allow_db_access = ...
    def __init__(self) -> None: ...
    def __getattr__(self, item):  # -> Any:
        ...
    def remove(*args, **kwargs):  # -> None:
        ...
    @staticmethod
    def set_allow_db_access(session, flag: bool):  # -> None:
        """Temporarily, e.g. for pytests allow access to DB to prepare stuff."""
        ...

    def is_called_from_test_code(self) -> tuple[bool, traceback.FrameSummary | None]:
        """
        Check if the traceback session was used from the test code.

        This is done by checking if the first "airflow" filename in the traceback
        is "airflow/tests" or "regular airflow".

        :meta: private
        :return: True if the object was created from test code, False otherwise.
        """
        ...

    def get_bind(
        self,
        mapper=...,
        clause=...,
        bind=...,
        _sa_skip_events=...,
        _sa_skip_for_implicit_returning=...,
    ):  # -> None:
        ...

def configure_orm(disable_connection_pool=..., pool_class=...):  # -> None:
    """Configure ORM using SQLAlchemy."""
    ...

def force_traceback_session_for_untrusted_components(
    allow_tests_to_use_db=...,
):  # -> None:
    ...

DEFAULT_ENGINE_ARGS = ...

def prepare_engine_args(
    disable_connection_pool=..., pool_class=...
):  # -> dict[Any, Any]:
    """Prepare SQLAlchemy engine args."""
    ...

def dispose_orm():  # -> None:
    """Properly close pooled database connections."""
    ...

def reconfigure_orm(disable_connection_pool=..., pool_class=...):  # -> None:
    """Properly close database connections and re-configure ORM."""
    ...

def configure_adapters():  # -> None:
    """Register Adapters and DB Converters."""
    ...

def validate_session():  # -> bool:
    """Validate ORM Session."""
    ...

def configure_action_logging() -> None:
    """Any additional configuration (register callback) for airflow.utils.action_loggers module."""
    ...

def prepare_syspath_for_config_and_plugins():  # -> None:
    """Update sys.path for the config and plugins directories."""
    ...

def prepare_syspath_for_dags_folder():  # -> None:
    """Update sys.path to include the DAGs folder."""
    ...

def get_session_lifetime_config():  # -> int:
    """Get session timeout configs and handle outdated configs gracefully."""
    ...

def import_local_settings():  # -> None:
    """Import airflow_local_settings.py files to allow overriding any configs in settings.py file."""
    ...

def initialize():  # -> None:
    """Initialize Airflow with all the settings from this file."""
    ...

def is_usage_data_collection_enabled() -> bool:
    """Check if data collection is enabled."""
    ...

KILOBYTE = ...
MEGABYTE = ...
WEB_COLORS = ...
MIN_SERIALIZED_DAG_UPDATE_INTERVAL = ...
COMPRESS_SERIALIZED_DAGS = ...
MIN_SERIALIZED_DAG_FETCH_INTERVAL = ...
CAN_FORK = ...
EXECUTE_TASKS_NEW_PYTHON_INTERPRETER = ...
ALLOW_FUTURE_EXEC_DATES = ...
CHECK_SLAS = ...
USE_JOB_SCHEDULE = ...
LAZY_LOAD_PLUGINS: bool = ...
LAZY_LOAD_PROVIDERS: bool = ...
IS_K8S_OR_K8SCELERY_EXECUTOR = ...
IS_EXECUTOR_CONTAINER = ...
IS_K8S_EXECUTOR_POD = ...
HIDE_SENSITIVE_VAR_CONN_FIELDS = ...
MASK_SECRETS_IN_LOGS = ...
DASHBOARD_UIALERTS: list[UIAlert] = ...
AIRFLOW_MOVED_TABLE_PREFIX = ...
DAEMON_UMASK: str = ...
_ENABLE_AIP_44: bool = ...
