"""
This type stub file was generated by pyright.
"""

from collections.abc import Sequence
from dataclasses import dataclass
from datetime import datetime
from typing import TYPE_CHECKING, Any

from airflow.callbacks.base_callback_sink import BaseCallbackSink
from airflow.callbacks.callback_requests import CallbackRequest
from airflow.cli.cli_config import GroupCommand
from airflow.exceptions import RemovedInAirflow3Warning
from airflow.executors.executor_utils import ExecutorName
from airflow.models.taskinstance import TaskInstance
from airflow.models.taskinstancekey import TaskInstanceKey
from airflow.traces.tracer import span
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.utils.state import TaskInstanceState
from deprecated import deprecated

"""Base executor - this is the base class for all the implemented executors."""
PARALLELISM: int = ...
if TYPE_CHECKING:
    CommandType = list[str]
    QueuedTaskInstanceType = tuple[CommandType, int, str | None, TaskInstance]
    EventBufferValueType = tuple[str | None, Any]
    TaskTuple = tuple[TaskInstanceKey, CommandType, str | None, Any | None]
log = ...

@dataclass
class RunningRetryAttemptType:
    """
    For keeping track of attempts to queue again when task still apparently running.

    We don't want to slow down the loop, so we don't block, but we allow it to be
    re-checked for at least MIN_SECONDS seconds.
    """

    MIN_SECONDS = ...
    total_tries: int = ...
    tries_after_min: int = ...
    first_attempt_time: datetime = ...
    @property
    def elapsed(self):
        """Seconds since first attempt."""
        ...

    def can_try_again(self):  # -> bool:
        """Return False if there has been at least one try greater than MIN_SECONDS, otherwise return True."""
        ...

class BaseExecutor(LoggingMixin):
    """
    Base class to inherit for concrete executors such as Celery, Kubernetes, Local, Sequential, etc.

    :param parallelism: how many jobs should run at one time. Set to ``0`` for infinity.
    """

    supports_ad_hoc_ti_run: bool = ...
    supports_pickling: bool = ...
    supports_sentry: bool = ...
    is_local: bool = ...
    is_single_threaded: bool = ...
    is_production: bool = ...
    change_sensor_mode_to_reschedule: bool = ...
    serve_logs: bool = ...
    job_id: None | int | str = ...
    name: None | ExecutorName = ...
    callback_sink: BaseCallbackSink | None = ...
    def __init__(self, parallelism: int = ...) -> None: ...
    def __repr__(self):  # -> str:
        ...
    def start(self):  # -> None:
        """Executors may need to get things started."""
        ...

    def log_task_event(
        self, *, event: str, extra: str, ti_key: TaskInstanceKey
    ):  # -> None:
        """Add an event to the log table."""
        ...

    def queue_command(
        self,
        task_instance: TaskInstance,
        command: CommandType,
        priority: int = ...,
        queue: str | None = ...,
    ):  # -> None:
        """Queues command to task."""
        ...

    def queue_task_instance(
        self,
        task_instance: TaskInstance,
        mark_success: bool = ...,
        pickle_id: int | None = ...,
        ignore_all_deps: bool = ...,
        ignore_depends_on_past: bool = ...,
        wait_for_past_depends_before_skipping: bool = ...,
        ignore_task_deps: bool = ...,
        ignore_ti_state: bool = ...,
        pool: str | None = ...,
        cfg_path: str | None = ...,
    ) -> None:
        """Queues task instance."""
        ...

    def has_task(self, task_instance: TaskInstance) -> bool:
        """
        Check if a task is either queued or running in this executor.

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor
        """
        ...

    def sync(self) -> None:
        """
        Sync will get called periodically by the heartbeat method.

        Executors should override this to perform gather statuses.
        """
        ...

    @span
    def heartbeat(self) -> None:
        """Heartbeat sent to trigger new jobs."""
        ...

    def order_queued_tasks_by_priority(
        self,
    ) -> list[tuple[TaskInstanceKey, QueuedTaskInstanceType]]:
        """
        Orders the queued tasks by priority.

        :return: List of tuples from the queued_tasks according to the priority.
        """
        ...

    @span
    def trigger_tasks(self, open_slots: int) -> None:
        """
        Initiate async execution of the queued tasks, up to the number of available slots.

        :param open_slots: Number of open slots
        """
        ...

    def change_state(
        self,
        key: TaskInstanceKey,
        state: TaskInstanceState,
        info=...,
        remove_running=...,
    ) -> None:
        """
        Change state of the task.

        :param key: Unique key for the task instance
        :param state: State to set for the task.
        :param info: Executor information for the task instance
        :param remove_running: Whether or not to remove the TI key from running set
        """
        ...

    def fail(self, key: TaskInstanceKey, info=...) -> None:
        """
        Set fail state for the event.

        :param info: Executor information for the task instance
        :param key: Unique key for the task instance
        """
        ...

    def success(self, key: TaskInstanceKey, info=...) -> None:
        """
        Set success state for the event.

        :param info: Executor information for the task instance
        :param key: Unique key for the task instance
        """
        ...

    def queued(self, key: TaskInstanceKey, info=...) -> None:
        """
        Set queued state for the event.

        :param info: Executor information for the task instance
        :param key: Unique key for the task instance
        """
        ...

    def running_state(self, key: TaskInstanceKey, info=...) -> None:
        """
        Set running state for the event.

        :param info: Executor information for the task instance
        :param key: Unique key for the task instance
        """
        ...

    def get_event_buffer(
        self, dag_ids=...
    ) -> dict[TaskInstanceKey, EventBufferValueType]:
        """
        Return and flush the event buffer.

        In case dag_ids is specified it will only return and flush events
        for the given dag_ids. Otherwise, it returns and flushes all events.

        :param dag_ids: the dag_ids to return events for; returns all if given ``None``.
        :return: a dict of events
        """
        ...

    def execute_async(
        self,
        key: TaskInstanceKey,
        command: CommandType,
        queue: str | None = ...,
        executor_config: Any | None = ...,
    ) -> None:
        """
        Execute the command asynchronously.

        :param key: Unique key for the task instance
        :param command: Command to run
        :param queue: name of the queue
        :param executor_config: Configuration passed to the executor.
        """
        ...

    def get_task_log(
        self, ti: TaskInstance, try_number: int
    ) -> tuple[list[str], list[str]]:
        """
        Return the task logs.

        :param ti: A TaskInstance object
        :param try_number: current try_number to read log from
        :return: tuple of logs and messages
        """
        ...

    def end(self) -> None:
        """Wait synchronously for the previously submitted job to complete."""
        ...

    def terminate(self):
        """Get called when the daemon receives a SIGTERM."""
        ...

    @deprecated(
        reason="Replaced by function `revoke_task`.",
        category=RemovedInAirflow3Warning,
        action="ignore",
    )
    def cleanup_stuck_queued_tasks(self, tis: list[TaskInstance]) -> list[str]:
        """
        Handle remnants of tasks that were failed because they were stuck in queued.

        Tasks can get stuck in queued. If such a task is detected, it will be marked
        as `UP_FOR_RETRY` if the task instance has remaining retries or marked as `FAILED`
        if it doesn't.

        :param tis: List of Task Instances to clean up
        :return: List of readable task instances for a warning message
        """
        ...

    def revoke_task(self, *, ti: TaskInstance):
        """
        Attempt to remove task from executor.

        It should attempt to ensure that the task is no longer running on the worker,
        and ensure that it is cleared out from internal data structures.

        It should *not* change the state of the task in airflow, or add any events
        to the event buffer.

        It should not raise any error.

        :param ti: Task instance to remove
        """
        ...

    def try_adopt_task_instances(
        self, tis: Sequence[TaskInstance]
    ) -> Sequence[TaskInstance]:
        """
        Try to adopt running task instances that have been abandoned by a SchedulerJob dying.

        Anything that is not adopted will be cleared by the scheduler (and then become eligible for
        re-scheduling)

        :return: any TaskInstances that were unable to be adopted
        """
        ...

    @property
    def slots_available(self):  # -> int:
        """Number of new tasks this executor instance can accept."""
        ...

    @property
    def slots_occupied(self):  # -> int:
        """Number of tasks this executor instance is currently managing."""
        ...

    @staticmethod
    def validate_command(command: list[str]) -> None:
        """
        Back-compat method to Check if the command to execute is airflow command.

        :param command: command to check
        """
        ...

    @staticmethod
    def validate_airflow_tasks_run_command(
        command: list[str],
    ) -> tuple[str | None, str | None]:
        """
        Check if the command to execute is airflow command.

        Returns tuple (dag_id,task_id) retrieved from the command (replaced with None values if missing)
        """
        ...

    def debug_dump(self):  # -> None:
        """Get called in response to SIGUSR2 by the scheduler."""
        ...

    def send_callback(self, request: CallbackRequest) -> None:
        """
        Send callback for execution.

        Provides a default implementation which sends the callback to the `callback_sink` object.

        :param request: Callback request to be executed.
        """
        ...

    @staticmethod
    def get_cli_commands() -> list[GroupCommand]:
        """
        Vends CLI commands to be included in Airflow CLI.

        Override this method to expose commands via Airflow CLI to manage this executor. This can
        be commands to setup/teardown the executor, inspect state, etc.
        Make sure to choose unique names for those commands, to avoid collisions.
        """
        ...
