"""
This type stub file was generated by pyright.
"""

import enum
import os
from datetime import datetime, timedelta
from multiprocessing.connection import Connection as MultiprocessingConnection
from typing import TYPE_CHECKING, NamedTuple

from airflow.api_internal.internal_api_call import internal_api_call
from airflow.traces.tracer import span
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.utils.mixins import MultiprocessingStartMethodMixin
from airflow.utils.session import provide_session
from sqlalchemy.orm import Session

"""Processes DAGs."""
if TYPE_CHECKING: ...

class DagParsingStat(NamedTuple):
    """Information on processing progress."""

    done: bool
    all_files_processed: bool
    ...

class DagFileStat(NamedTuple):
    """Information about single processing of one file."""

    num_dags: int
    import_errors: int
    last_finish_time: datetime | None
    last_duration: timedelta | None
    run_count: int
    last_num_of_db_queries: int
    ...

class DagParsingSignal(enum.Enum):
    """All signals sent to parser."""

    AGENT_RUN_ONCE = ...
    TERMINATE_MANAGER = ...
    END_MANAGER = ...

class DagFileProcessorAgent(LoggingMixin, MultiprocessingStartMethodMixin):
    """
    Agent for DAG file processing.

    It is responsible for all DAG parsing related jobs in scheduler process.
    Mainly it can spin up DagFileProcessorManager in a subprocess,
    collect DAG parsing results from it and communicate signal/DAG parsing stat with it.

    This class runs in the main `airflow scheduler` process.

    :param dag_directory: Directory where DAG definitions are kept. All
        files in file_paths should be under this directory
    :param max_runs: The number of times to parse and schedule each file. -1
        for unlimited.
    :param processor_timeout: How long to wait before timing out a DAG file processor
    :param dag_ids: if specified, only schedule tasks with these DAG IDs
    :param pickle_dags: whether to pickle DAGs.
    :param async_mode: Whether to start agent in async mode
    """

    def __init__(
        self,
        dag_directory: os.PathLike,
        max_runs: int,
        processor_timeout: timedelta,
        dag_ids: list[str] | None,
        pickle_dags: bool,
        async_mode: bool,
    ) -> None: ...
    def start(self) -> None:
        """Launch DagFileProcessorManager processor and start DAG parsing loop in manager."""
        ...

    def run_single_parsing_loop(self) -> None:
        """
        Send agent heartbeat signal to the manager, requesting that it runs one processing "loop".

        Should only be used when launched DAG file processor manager in sync mode.

        Call wait_until_finished to ensure that any launched processors have finished before continuing.
        """
        ...

    def get_callbacks_pipe(self) -> MultiprocessingConnection:
        """Return the pipe for sending Callbacks to DagProcessorManager."""
        ...

    def wait_until_finished(self) -> None:
        """Wait until DAG parsing is finished."""
        ...

    def heartbeat(self) -> None:
        """Check if the DagFileProcessorManager process is alive, and process any pending messages."""
        ...

    @property
    def done(self) -> bool:
        """Whether the DagFileProcessorManager finished."""
        ...

    @property
    def all_files_processed(self):  # -> bool:
        """Whether all files been processed at least once."""
        ...

    def terminate(self):  # -> None:
        """Send termination signal to DAG parsing processor manager to terminate all DAG file processors."""
        ...

    def end(self):  # -> None:
        """Terminate (and then kill) the manager process launched."""
        ...

class DagFileProcessorManager(LoggingMixin):
    """
    Manage processes responsible for parsing DAGs.

    Given a list of DAG definition files, this kicks off several processors
    in parallel to process them and put the results to a multiprocessing.Queue
    for DagFileProcessorAgent to harvest. The parallelism is limited and as the
    processors finish, more are launched. The files are processed over and
    over again, but no more often than the specified interval.

    :param dag_directory: Directory where DAG definitions are kept. All
        files in file_paths should be under this directory
    :param max_runs: The number of times to parse and schedule each file. -1
        for unlimited.
    :param processor_timeout: How long to wait before timing out a DAG file processor
    :param signal_conn: connection to communicate signal with processor agent.
    :param dag_ids: if specified, only schedule tasks with these DAG IDs
    :param pickle_dags: whether to pickle DAGs.
    :param async_mode: whether to start the manager in async mode
    """

    DEFAULT_FILE_STAT = ...
    def __init__(
        self,
        dag_directory: os.PathLike[str],
        max_runs: int,
        processor_timeout: timedelta,
        dag_ids: list[str] | None,
        pickle_dags: bool,
        signal_conn: MultiprocessingConnection | None = ...,
        async_mode: bool = ...,
    ) -> None: ...
    def register_exit_signals(self):  # -> None:
        """Register signals that stop child processes."""
        ...

    def start(self):  # -> None:
        """
        Use multiple processes to parse and generate tasks for the DAGs in parallel.

        By processing them in separate processes, we can get parallelism and isolation
        from potentially harmful user code.
        """
        ...

    @classmethod
    @internal_api_call
    @provide_session
    def deactivate_stale_dags(
        cls,
        last_parsed: dict[str, datetime | None],
        dag_directory: str,
        stale_dag_threshold: int,
        session: Session = ...,
    ):  # -> None:
        """
        Detect DAGs which are no longer present in files.

        Deactivate them and remove them in the serialized_dag table.
        """
        ...

    @staticmethod
    @internal_api_call
    @provide_session
    def clear_nonexistent_import_errors(
        file_paths: list[str] | None, processor_subdir: str | None, session=...
    ):  # -> None:
        """
        Clear import errors for files that no longer exist.

        :param file_paths: list of paths to DAG definition files
        :param session: session for ORM operations
        """
        ...

    def get_pid(self, file_path) -> int | None:
        """
        Retrieve the PID of the process processing the given file or None if the file is not being processed.

        :param file_path: the path to the file that's being processed.
        """
        ...

    def get_all_pids(self) -> list[int]:
        """
        Get all pids.

        :return: a list of the PIDs for the processors that are running
        """
        ...

    def get_last_runtime(self, file_path) -> float | None:
        """
        Retrieve the last processing time of a specific path.

        :param file_path: the path to the file that was processed
        :return: the runtime (in seconds) of the process of the last run, or
            None if the file was never processed.
        """
        ...

    def get_last_dag_count(self, file_path) -> int | None:
        """
        Retrieve the total DAG count at a specific path.

        :param file_path: the path to the file that was processed
        :return: the number of dags loaded from that file, or None if the file was never processed.
        """
        ...

    def get_last_error_count(self, file_path) -> int | None:
        """
        Retrieve the total number of errors from processing a specific path.

        :param file_path: the path to the file that was processed
        :return: the number of import errors from processing, or None if the file was never processed.
        """
        ...

    def get_last_num_of_db_queries(self, file_path) -> int | None:
        """
        Retrieve the number of queries performed to the Airflow database during last parsing of the file.

        :param file_path: the path to the file that was processed
        :return: the number of queries performed to the Airflow database during last parsing of the file,
            or None if the file was never processed.
        """
        ...

    def get_last_finish_time(self, file_path) -> datetime | None:
        """
        Retrieve the last completion time for processing a specific path.

        :param file_path: the path to the file that was processed
        :return: the finish time of the process of the last run, or None if the file was never processed.
        """
        ...

    def get_start_time(self, file_path) -> datetime | None:
        """
        Retrieve the last start time for processing a specific path.

        :param file_path: the path to the file that's being processed
        :return: the start time of the process that's processing the
            specified file or None if the file is not currently being processed.
        """
        ...

    def get_run_count(self, file_path) -> int:
        """
        Return the number of times the given file has been parsed.

        :param file_path: the path to the file that's being processed.
        """
        ...

    def get_dag_directory(self) -> str:
        """Return the dag_director as a string."""
        ...

    def set_file_paths(self, new_file_paths):  # -> None:
        """
        Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :return: None
        """
        ...

    def wait_until_finished(self):  # -> None:
        """Sleeps until all the processors are done."""
        ...

    def collect_results(self) -> None:
        """Collect the result from any finished DAG processors."""
        ...

    @span
    def start_new_processes(self):  # -> None:
        """Start more processors if we have enough slots and files to process."""
        ...

    @span
    def add_new_file_path_to_queue(self):  # -> None:
        ...
    def prepare_file_path_queue(self):  # -> None:
        """
        Scan dags dir to generate more file paths to process.

        Note this method is only called when the file path queue is empty
        """
        ...

    def max_runs_reached(self):  # -> bool:
        """:return: whether all file paths have been processed max_runs times."""
        ...

    def terminate(self):  # -> None:
        """Stop all running processors."""
        ...

    def end(self):  # -> None:
        """Kill all child processes on exit since we don't want to leave them as orphaned."""
        ...

    def emit_metrics(self):  # -> None:
        """
        Emit metrics about dag parsing summary.

        This is called once every time around the parsing "loop" - i.e. after
        all files have been parsed.
        """
        ...

    @property
    def file_paths(self):  # -> list[str]:
        ...

def reload_configuration_for_dag_processing():  # -> None:
    ...
