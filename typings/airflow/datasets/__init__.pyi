"""
This type stub file was generated by pyright.
"""

import os
from collections.abc import Callable, Iterable, Iterator
from typing import TYPE_CHECKING, Any, ClassVar
from urllib.parse import SplitResult

import attr
from airflow.api_internal.internal_api_call import internal_api_call
from airflow.serialization.dag_dependency import DagDependency
from airflow.typing_compat import TypedDict
from airflow.utils.session import provide_session
from sqlalchemy.orm.session import Session

if TYPE_CHECKING: ...
__all__ = ["Dataset", "DatasetAll", "DatasetAny"]

def normalize_noop(parts: SplitResult) -> SplitResult:
    """
    Place-hold a :class:`~urllib.parse.SplitResult`` normalizer.

    :meta private:
    """
    ...

def extract_event_key(value: str | Dataset | DatasetAlias) -> str:
    """
    Extract the key of an inlet or an outlet event.

    If the input value is a string, it is treated as a URI and sanitized. If the
    input is a :class:`Dataset`, the URI it contains is considered sanitized and
    returned directly. If the input is a :class:`DatasetAlias`, the name it contains
    will be returned directly.

    :meta private:
    """
    ...

@internal_api_call
@provide_session
def expand_alias_to_datasets(
    alias: str | DatasetAlias, *, session: Session = ...
) -> list[BaseDataset]:
    """Expand dataset alias to resolved datasets."""
    ...

class BaseDataset:
    """
    Protocol for all dataset triggers to use in ``DAG(schedule=...)``.

    :meta private:
    """

    def __bool__(self) -> bool: ...
    def __or__(self, other: BaseDataset) -> BaseDataset: ...
    def __and__(self, other: BaseDataset) -> BaseDataset: ...
    def as_expression(self) -> Any:
        """
        Serialize the dataset into its scheduling expression.

        The return value is stored in DagModel for display purposes. It must be
        JSON-compatible.

        :meta private:
        """
        ...

    def evaluate(self, statuses: dict[str, bool]) -> bool: ...
    def iter_datasets(self) -> Iterator[tuple[str, Dataset]]: ...
    def iter_dataset_aliases(self) -> Iterator[DatasetAlias]: ...
    def iter_dag_dependencies(
        self, *, source: str, target: str
    ) -> Iterator[DagDependency]:
        """
        Iterate a base dataset as dag dependency.

        :meta private:
        """
        ...

@attr.define()
class DatasetAlias(BaseDataset):
    """A represeation of dataset alias which is used to create dataset during the runtime."""

    name: str
    def __eq__(self, other: Any) -> bool: ...
    def __hash__(self) -> int: ...
    def iter_dag_dependencies(
        self, *, source: str, target: str
    ) -> Iterator[DagDependency]:
        """
        Iterate a dataset alias as dag dependency.

        :meta private:
        """
        ...

class DatasetAliasEvent(TypedDict):
    """A represeation of dataset event to be triggered by a dataset alias."""

    source_alias_name: str
    dest_dataset_uri: str
    extra: dict[str, Any]
    ...

@attr.define()
class Dataset(os.PathLike, BaseDataset):
    """A representation of data dependencies between workflows."""

    uri: str = ...
    extra: dict[str, Any] | None = ...
    __version__: ClassVar[int] = ...
    def __fspath__(self) -> str: ...
    def __eq__(self, other: Any) -> bool: ...
    def __hash__(self) -> int: ...
    @property
    def normalized_uri(self) -> str | None:
        """
        Returns the normalized and AIP-60 compliant URI whenever possible.

        If we can't retrieve the scheme from URI or no normalizer is provided or if parsing fails,
        it returns None.

        If a normalizer for the scheme exists and parsing is successful we return the normalizer result.
        """
        ...

    def as_expression(self) -> Any:
        """
        Serialize the dataset into its scheduling expression.

        :meta private:
        """
        ...

    def iter_datasets(self) -> Iterator[tuple[str, Dataset]]: ...
    def iter_dataset_aliases(self) -> Iterator[DatasetAlias]: ...
    def evaluate(self, statuses: dict[str, bool]) -> bool: ...
    def iter_dag_dependencies(
        self, *, source: str, target: str
    ) -> Iterator[DagDependency]:
        """
        Iterate a dataset as dag dependency.

        :meta private:
        """
        ...

class _DatasetBooleanCondition(BaseDataset):
    """Base class for dataset boolean logic."""

    agg_func: Callable[[Iterable], bool]
    def __init__(self, *objects: BaseDataset) -> None: ...
    def evaluate(self, statuses: dict[str, bool]) -> bool: ...
    def iter_datasets(self) -> Iterator[tuple[str, Dataset]]: ...
    def iter_dataset_aliases(self) -> Iterator[DatasetAlias]:
        """Filter dataest aliases in the condition."""
        ...

    def iter_dag_dependencies(
        self, *, source: str, target: str
    ) -> Iterator[DagDependency]:
        """
        Iterate dataset, dataset aliases and their resolved datasets  as dag dependency.

        :meta private:
        """
        ...

class DatasetAny(_DatasetBooleanCondition):
    """Use to combine datasets schedule references in an "and" relationship."""

    agg_func = ...
    def __or__(self, other: BaseDataset) -> BaseDataset: ...
    def __repr__(self) -> str: ...
    def as_expression(self) -> dict[str, Any]:
        """
        Serialize the dataset into its scheduling expression.

        :meta private:
        """
        ...

class _DatasetAliasCondition(DatasetAny):
    """
    Use to expand DataAlias as DatasetAny of its resolved Datasets.

    :meta private:
    """

    def __init__(self, name: str) -> None: ...
    def __repr__(self) -> str: ...
    def as_expression(self) -> Any:
        """
        Serialize the dataset into its scheduling expression.

        :meta private:
        """
        ...

    def iter_dataset_aliases(self) -> Iterator[DatasetAlias]: ...
    def iter_dag_dependencies(
        self, *, source: str = ..., target: str = ...
    ) -> Iterator[DagDependency]:
        """
        Iterate a dataset alias and its resolved datasets  as dag dependency.

        :meta private:
        """
        ...

class DatasetAll(_DatasetBooleanCondition):
    """Use to combine datasets schedule references in an "or" relationship."""

    agg_func = ...
    def __and__(self, other: BaseDataset) -> BaseDataset: ...
    def __repr__(self) -> str: ...
    def as_expression(self) -> Any:
        """
        Serialize the dataset into its scheduling expression.

        :meta private:
        """
        ...
