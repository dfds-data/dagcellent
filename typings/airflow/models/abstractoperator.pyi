"""
This type stub file was generated by pyright.
"""

import datetime
from abc import abstractproperty
from collections.abc import Callable, Collection, Iterable, Iterator, Sequence
from functools import cached_property
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
)

import jinja2
import methodtools
from airflow.models.baseoperator import BaseOperator
from airflow.models.baseoperatorlink import BaseOperatorLink
from airflow.models.dag import DAG
from airflow.models.mappedoperator import MappedOperator
from airflow.models.operator import Operator
from airflow.models.taskinstance import TaskInstance
from airflow.models.taskmixin import DAGNode
from airflow.task.priority_strategy import PriorityWeightStrategy
from airflow.template.templater import Templater
from airflow.triggers.base import StartTriggerArgs
from airflow.utils.context import Context
from airflow.utils.task_group import MappedTaskGroup
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.types import ArgNotSet
from airflow.utils.weight_rule import WeightRule
from sqlalchemy.orm import Session

TaskStateChangeCallback = Callable[[Context], None]
if TYPE_CHECKING: ...
DEFAULT_OWNER: str = ...
DEFAULT_POOL_SLOTS: int = ...
DEFAULT_PRIORITY_WEIGHT: int = ...
DEFAULT_EXECUTOR: str | None = ...
DEFAULT_QUEUE: str = ...
DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST: bool = ...
DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING: bool = ...
DEFAULT_RETRIES: int = ...
DEFAULT_RETRY_DELAY: datetime.timedelta = ...
MAX_RETRY_DELAY: int = ...
DEFAULT_WEIGHT_RULE: WeightRule = ...
DEFAULT_TRIGGER_RULE: TriggerRule = ...
DEFAULT_TASK_EXECUTION_TIMEOUT: datetime.timedelta | None = ...

class NotMapped(Exception):
    """Raise if a task is neither mapped nor has any parent mapped groups."""

    ...

class AbstractOperator(Templater, DAGNode):
    """
    Common implementation for operators, including unmapped and mapped.

    This base class is more about sharing implementations, not defining a common
    interface. Unfortunately it's difficult to use this as the common base class
    for typing due to BaseOperator carrying too much historical baggage.

    The union type ``from airflow.models.operator import Operator`` is easier
    to use for typing purposes.

    :meta private:
    """

    operator_class: type[BaseOperator] | dict[str, Any]
    weight_rule: PriorityWeightStrategy
    priority_weight: int
    operator_extra_links: Collection[BaseOperatorLink]
    owner: str
    task_id: str
    outlets: list
    inlets: list
    trigger_rule: TriggerRule
    _needs_expansion: bool | None = ...
    _on_failure_fail_dagrun = ...
    HIDE_ATTRS_FROM_UI: ClassVar[frozenset[str]] = ...
    def get_dag(self) -> DAG | None: ...
    @property
    def task_type(self) -> str: ...
    @property
    def operator_name(self) -> str: ...
    @property
    def inherits_from_empty_operator(self) -> bool: ...
    @property
    def dag_id(self) -> str:
        """Returns dag id if it has one or an adhoc + owner."""
        ...

    @property
    def node_id(self) -> str: ...
    @abstractproperty
    def task_display_name(self) -> str: ...
    @property
    def label(self) -> str | None: ...
    @property
    def is_setup(self) -> bool: ...
    @is_setup.setter
    def is_setup(self, value: bool) -> None: ...
    @property
    def is_teardown(self) -> bool: ...
    @is_teardown.setter
    def is_teardown(self, value: bool) -> None: ...
    @property
    def on_failure_fail_dagrun(self):  # -> bool:
        """
        Whether the operator should fail the dagrun on failure.

        :meta private:
        """
        ...

    @on_failure_fail_dagrun.setter
    def on_failure_fail_dagrun(self, value):  # -> None:
        """
        Setter for on_failure_fail_dagrun property.

        :meta private:
        """
        ...

    def as_setup(self):  # -> Self:
        ...
    def as_teardown(
        self,
        *,
        setups: BaseOperator | Iterable[BaseOperator] | ArgNotSet = ...,
        on_failure_fail_dagrun=...,
    ):  # -> Self:
        ...
    def get_direct_relative_ids(self, upstream: bool = ...) -> set[str]:
        """Get direct relative IDs to the current task, upstream or downstream."""
        ...

    def get_flat_relative_ids(self, *, upstream: bool = ...) -> set[str]:
        """
        Get a flat set of relative IDs, upstream or downstream.

        Will recurse each relative found in the direction specified.

        :param upstream: Whether to look for upstream or downstream relatives.
        """
        ...

    def get_flat_relatives(self, upstream: bool = ...) -> Collection[Operator]:
        """Get a flat list of relatives, either upstream or downstream."""
        ...

    def get_upstreams_follow_setups(self) -> Iterable[Operator]:
        """All upstreams and, for each upstream setup, its respective teardowns."""
        ...

    def get_upstreams_only_setups_and_teardowns(self) -> Iterable[Operator]:
        """
        Only *relevant* upstream setups and their teardowns.

        This method is meant to be used when we are clearing the task (non-upstream) and we need
        to add in the *relevant* setups and their teardowns.

        Relevant in this case means, the setup has a teardown that is downstream of ``self``,
        or the setup has no teardowns.
        """
        ...

    def get_upstreams_only_setups(self) -> Iterable[Operator]:
        """
        Return relevant upstream setups.

        This method is meant to be used when we are checking task dependencies where we need
        to wait for all the upstream setups to complete before we can run the task.
        """
        ...

    def iter_mapped_dependants(self) -> Iterator[MappedOperator | MappedTaskGroup]:
        """
        Return mapped nodes that depend on the current task the expansion.

        For now, this walks the entire DAG to find mapped nodes that has this
        current task as an upstream. We cannot use ``downstream_list`` since it
        only contains operators, not task groups. In the future, we should
        provide a way to record an DAG node's all downstream nodes instead.
        """
        ...

    def iter_mapped_task_groups(self) -> Iterator[MappedTaskGroup]:
        """
        Return mapped task groups this task belongs to.

        Groups are returned from the innermost to the outmost.

        :meta private:
        """
        ...

    def get_closest_mapped_task_group(self) -> MappedTaskGroup | None:
        """
        Get the mapped task group "closest" to this task in the DAG.

        :meta private:
        """
        ...

    def get_needs_expansion(self) -> bool:
        """
        Return true if the task is MappedOperator or is in a mapped task group.

        :meta private:
        """
        ...

    def unmap(
        self, resolve: None | dict[str, Any] | tuple[Context, Session]
    ) -> BaseOperator:
        """
        Get the "normal" operator from current abstract operator.

        MappedOperator uses this to unmap itself based on the map index. A non-
        mapped operator (i.e. BaseOperator subclass) simply returns itself.

        :meta private:
        """
        ...

    def expand_start_from_trigger(self, *, context: Context, session: Session) -> bool:
        """
        Get the start_from_trigger value of the current abstract operator.

        MappedOperator uses this to unmap start_from_trigger to decide whether to start the task
        execution directly from triggerer.

        :meta private:
        """
        ...

    def expand_start_trigger_args(
        self, *, context: Context, session: Session
    ) -> StartTriggerArgs | None:
        """
        Get the start_trigger_args value of the current abstract operator.

        MappedOperator uses this to unmap start_trigger_args to decide how to start a task from triggerer.

        :meta private:
        """
        ...

    @property
    def priority_weight_total(self) -> int:
        """
        Total priority weight for the task. It might include all upstream or downstream tasks.

        Depending on the weight rule:

        - WeightRule.ABSOLUTE - only own weight
        - WeightRule.DOWNSTREAM - adds priority weight of all downstream tasks
        - WeightRule.UPSTREAM - adds priority weight of all upstream tasks
        """
        ...

    @cached_property
    def operator_extra_link_dict(self) -> dict[str, Any]:
        """Returns dictionary of all extra links for the operator."""
        ...

    @cached_property
    def global_operator_extra_link_dict(self) -> dict[str, Any]:
        """Returns dictionary of all global extra links."""
        ...

    @cached_property
    def extra_links(self) -> list[str]: ...
    def get_extra_links(self, ti: TaskInstance, link_name: str) -> str | None:
        """
        For an operator, gets the URLs that the ``extra_links`` entry points to.

        :meta private:

        :raise ValueError: The error message of a ValueError will be passed on through to
            the fronted to show up as a tooltip on the disabled link.
        :param ti: The TaskInstance for the URL being searched for.
        :param link_name: The name of the link we're looking for the URL for. Should be
            one of the options specified in ``extra_links``.
        """
        ...

    @methodtools.lru_cache(maxsize=None)
    def get_parse_time_mapped_ti_count(self) -> int:
        """
        Return the number of mapped task instances that can be created on DAG run creation.

        This only considers literal mapped arguments, and would return *None*
        when any non-literal values are used for mapping.

        :raise NotFullyPopulated: If non-literal mapped arguments are encountered.
        :raise NotMapped: If the operator is neither mapped, nor has any parent
            mapped task groups.
        :return: Total number of mapped TIs this task should have.
        """
        ...

    def get_mapped_ti_count(self, run_id: str, *, session: Session) -> int:
        """
        Return the number of mapped TaskInstances that can be created at run time.

        This considers both literal and non-literal mapped arguments, and the
        result is therefore available when all depended tasks have finished. The
        return value should be identical to ``parse_time_mapped_ti_count`` if
        all mapped arguments are literal.

        :raise NotFullyPopulated: If upstream tasks are not all complete yet.
        :raise NotMapped: If the operator is neither mapped, nor has any parent
            mapped task groups.
        :return: Total number of mapped TIs this task should have.
        """
        ...

    def expand_mapped_task(
        self, run_id: str, *, session: Session
    ) -> tuple[Sequence[TaskInstance], int]:
        """
        Create the mapped task instances for mapped task.

        :raise NotMapped: If this task does not need expansion.
        :return: The newly created mapped task instances (if any) in ascending
            order by map index, and the maximum map index value.
        """
        ...

    def render_template_fields(
        self, context: Context, jinja_env: jinja2.Environment | None = ...
    ) -> None:
        """
        Template all attributes listed in *self.template_fields*.

        If the operator is mapped, this should return the unmapped, fully
        rendered, and map-expanded operator. The mapped operator should not be
        modified. However, *context* may be modified in-place to reference the
        unmapped operator for template rendering.

        If the operator is not mapped, this should modify the operator in-place.
        """
        ...

    def get_template_env(self, dag: DAG | None = ...) -> jinja2.Environment:
        """Get the template environment for rendering templates."""
        ...

    def __enter__(self):  # -> type[SetupTeardownContext]:
        ...
    def __exit__(self, exc_type, exc_val, exc_tb):  # -> None:
        ...
