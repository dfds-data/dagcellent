"""
This type stub file was generated by pyright.
"""

from collections.abc import Collection, Iterable, Mapping
from inspect import Parameter
from typing import TYPE_CHECKING, Any, NamedTuple, Union

from airflow.compat.functools import cache
from airflow.datasets import BaseDataset
from airflow.exceptions import AirflowException
from airflow.models.baseoperator import BaseOperator
from airflow.models.dag import DAG
from airflow.models.expandinput import ExpandInput
from airflow.models.mappedoperator import MappedOperator
from airflow.models.operator import Operator
from airflow.models.xcom_arg import XComArg
from airflow.serialization.dag_dependency import DagDependency
from airflow.serialization.enums import DagAttributeTypes as DAT, Encoding
from airflow.serialization.json_schema import Validator
from airflow.task.priority_strategy import PriorityWeightStrategy
from airflow.timetables.base import Timetable
from airflow.triggers.base import StartTriggerArgs
from airflow.utils.context import OutletEventAccessor
from airflow.utils.task_group import TaskGroup
from dateutil import relativedelta
from pendulum.tz.timezone import FixedTimezone, Timezone

"""Serialized DAG and BaseOperator."""
if TYPE_CHECKING:
    HAS_KUBERNETES: bool
    ...
log = ...
_OPERATOR_EXTRA_LINKS: set[str] = ...

@cache
def get_operator_extra_links() -> set[str]:
    """
    Get the operator extra links.

    This includes both the built-in ones, and those come from the providers.
    """
    ...

def encode_relativedelta(var: relativedelta.relativedelta) -> dict[str, Any]:
    """Encode a relativedelta object."""
    ...

def decode_relativedelta(var: dict[str, Any]) -> relativedelta.relativedelta:
    """Dencode a relativedelta object."""
    ...

def encode_timezone(var: Timezone | FixedTimezone) -> str | int:
    """
    Encode a Pendulum Timezone for serialization.

    Airflow only supports timezone objects that implements Pendulum's Timezone
    interface. We try to keep as much information as possible to make conversion
    round-tripping possible (see ``decode_timezone``). We need to special-case
    UTC; Pendulum implements it as a FixedTimezone (i.e. it gets encoded as
    0 without the special case), but passing 0 into ``pendulum.timezone`` does
    not give us UTC (but ``+00:00``).
    """
    ...

def decode_timezone(var: str | int) -> Timezone | FixedTimezone:
    """Decode a previously serialized Pendulum Timezone."""
    ...

class _TimetableNotRegistered(ValueError):
    def __init__(self, type_string: str) -> None: ...
    def __str__(self) -> str: ...

class _PriorityWeightStrategyNotRegistered(AirflowException):
    def __init__(self, type_string: str) -> None: ...
    def __str__(self) -> str: ...

def encode_dataset_condition(var: BaseDataset) -> dict[str, Any]:
    """
    Encode a dataset condition.

    :meta private:
    """
    ...

def decode_dataset_condition(var: dict[str, Any]) -> BaseDataset:
    """
    Decode a previously serialized dataset condition.

    :meta private:
    """
    ...

def encode_outlet_event_accessor(var: OutletEventAccessor) -> dict[str, Any]: ...
def decode_outlet_event_accessor(var: dict[str, Any]) -> OutletEventAccessor: ...
def encode_timetable(var: Timetable) -> dict[str, Any]:
    """
    Encode a timetable instance.

    This delegates most of the serialization work to the type, so the behavior
    can be completely controlled by a custom subclass.

    :meta private:
    """
    ...

def decode_timetable(var: dict[str, Any]) -> Timetable:
    """
    Decode a previously serialized timetable.

    Most of the deserialization logic is delegated to the actual type, which
    we import from string.

    :meta private:
    """
    ...

def encode_priority_weight_strategy(var: PriorityWeightStrategy) -> str:
    """
    Encode a priority weight strategy instance.

    In this version, we only store the importable string, so the class should not wait
    for any parameters to be passed to it. If you need to store the parameters, you
    should store them in the class itself.
    """
    ...

def decode_priority_weight_strategy(var: str) -> PriorityWeightStrategy:
    """
    Decode a previously serialized priority weight strategy.

    In this version, we only store the importable string, so we just need to get the class
    from the dictionary of registered classes and instantiate it with no parameters.
    """
    ...

def encode_start_trigger_args(var: StartTriggerArgs) -> dict[str, Any]:
    """
    Encode a StartTriggerArgs.

    :meta private:
    """
    ...

def decode_start_trigger_args(var: dict[str, Any]) -> StartTriggerArgs:
    """
    Decode a StartTriggerArgs.

    :meta private:
    """
    ...

class _XComRef(NamedTuple):
    """
    Store info needed to create XComArg.

    We can't turn it in to a XComArg until we've loaded _all_ the tasks, so when
    deserializing an operator, we need to create something in its place, and
    post-process it in ``deserialize_dag``.
    """

    data: dict
    def deref(self, dag: DAG) -> XComArg: ...

_ExpandInputOriginalValue = Union[
    Mapping[str, Any],
    XComArg,
    Collection[XComArg | Mapping[str, Any]],
]
_ExpandInputSerializedValue = Union[
    Mapping[str, Any],
    _XComRef,
    Collection[_XComRef | Mapping[str, Any]],
]

class _ExpandInputRef(NamedTuple):
    """
    Store info needed to create a mapped operator's expand input.

    This references a ``ExpandInput`` type, but replaces ``XComArg`` objects
    with ``_XComRef`` (see documentation on the latter type for reasoning).
    """

    key: str
    value: _ExpandInputSerializedValue
    @classmethod
    def validate_expand_input_value(cls, value: _ExpandInputOriginalValue) -> None:
        """
        Validate we've covered all ``ExpandInput.value`` types.

        This function does not actually do anything, but is called during
        serialization so Mypy will *statically* check we have handled all
        possible ExpandInput cases.
        """
        ...

    def deref(self, dag: DAG) -> ExpandInput:
        """
        De-reference into a concrete ExpandInput object.

        If you add more cases here, be sure to update _ExpandInputOriginalValue
        and _ExpandInputSerializedValue to match the logic.
        """
        ...

_orm_to_model = ...
_type_to_class: dict[DAT | str, list] = ...
_class_to_type = ...

def add_pydantic_class_type_mapping(
    attribute_type: str, orm_class, pydantic_class
):  # -> None:
    ...

class BaseSerialization:
    """BaseSerialization provides utils for serialization."""

    _primitive_types = ...
    _datetime_types = ...
    _excluded_types = ...
    _json_schema: Validator | None = ...
    _load_operator_extra_links = ...
    _CONSTRUCTOR_PARAMS: dict[str, Parameter] = ...
    SERIALIZER_VERSION = ...
    @classmethod
    def to_json(cls, var: DAG | BaseOperator | dict | list | set | tuple) -> str:
        """Stringify DAGs and operators contained by var and returns a JSON string of var."""
        ...

    @classmethod
    def to_dict(cls, var: DAG | BaseOperator | dict | list | set | tuple) -> dict:
        """Stringify DAGs and operators contained by var and returns a dict of var."""
        ...

    @classmethod
    def from_json(
        cls, serialized_obj: str
    ) -> BaseSerialization | dict | list | set | tuple:
        """Deserialize json_str and reconstructs all DAGs and operators it contains."""
        ...

    @classmethod
    def from_dict(
        cls, serialized_obj: dict[Encoding, Any]
    ) -> BaseSerialization | dict | list | set | tuple:
        """Deserialize a dict of type decorators and reconstructs all DAGs and operators it contains."""
        ...

    @classmethod
    def validate_schema(cls, serialized_obj: str | dict) -> None:
        """Validate serialized_obj satisfies JSON schema."""
        ...

    @classmethod
    def serialize_to_json(
        cls,
        object_to_serialize: BaseOperator | MappedOperator | DAG,
        decorated_fields: set,
    ) -> dict[str, Any]:
        """Serialize an object to JSON."""
        ...

    @classmethod
    def serialize(
        cls, var: Any, *, strict: bool = ..., use_pydantic_models: bool = ...
    ) -> Any:
        """
        Serialize an object; helper function of depth first search for serialization.

        The serialization protocol is:

        (1) keeping JSON supported types: primitives, dict, list;
        (2) encoding other types as ``{TYPE: 'foo', VAR: 'bar'}``, the deserialization
            step decode VAR according to TYPE;
        (3) Operator has a special field CLASS to record the original class
            name for displaying in UI.

        :meta private:
        """
        ...

    @classmethod
    def default_serialization(cls, strict, var) -> str: ...
    @classmethod
    def deserialize(cls, encoded_var: Any, use_pydantic_models=...) -> Any:
        """
        Deserialize an object; helper function of depth first search for deserialization.

        :meta private:
        """
        ...

    _deserialize_datetime = ...
    _deserialize_timezone = ...

class DependencyDetector:
    """
    Detects dependencies between DAGs.

    :meta private:
    """

    @staticmethod
    def detect_task_dependencies(task: Operator) -> list[DagDependency]:
        """Detect dependencies caused by tasks."""
        ...

    @staticmethod
    def detect_dag_dependencies(dag: DAG | None) -> Iterable[DagDependency]:
        """Detect dependencies set directly on the DAG object."""
        ...

class SerializedBaseOperator(BaseOperator, BaseSerialization):
    """
    A JSON serializable representation of operator.

    All operators are casted to SerializedBaseOperator after deserialization.
    Class specific attributes used by UI are move to object attributes.

    Creating a SerializedBaseOperator is a three-step process:

    1. Instantiate a :class:`SerializedBaseOperator` object.
    2. Populate attributes with :func:`SerializedBaseOperator.populated_operator`.
    3. When the task's containing DAG is available, fix references to the DAG
       with :func:`SerializedBaseOperator.set_task_dag_references`.
    """

    _decorated_fields = ...
    _CONSTRUCTOR_PARAMS = ...
    def __init__(self, *args, **kwargs) -> None: ...
    @property
    def task_type(self) -> str: ...
    @task_type.setter
    def task_type(self, task_type: str):  # -> None:
        ...
    @property
    def operator_name(self) -> str: ...
    @operator_name.setter
    def operator_name(self, operator_name: str):  # -> None:
        ...
    @classmethod
    def serialize_mapped_operator(cls, op: MappedOperator) -> dict[str, Any]: ...
    @classmethod
    def serialize_operator(
        cls, op: BaseOperator | MappedOperator
    ) -> dict[str, Any]: ...
    @classmethod
    def populate_operator(cls, op: Operator, encoded_op: dict[str, Any]) -> None:
        """
        Populate operator attributes with serialized values.

        This covers simple attributes that don't reference other things in the
        DAG. Setting references (such as ``op.dag`` and task dependencies) is
        done in ``set_task_dag_references`` instead, which is called after the
        DAG is hydrated.
        """
        ...

    @staticmethod
    def set_task_dag_references(task: Operator, dag: DAG) -> None:
        """
        Handle DAG references on an operator.

        The operator should have been mostly populated earlier by calling
        ``populate_operator``. This function further fixes object references
        that were not possible before the task's containing DAG is hydrated.
        """
        ...

    @classmethod
    def deserialize_operator(cls, encoded_op: dict[str, Any]) -> Operator:
        """Deserializes an operator from a JSON object."""
        ...

    @classmethod
    def detect_dependencies(cls, op: Operator) -> set[DagDependency]:
        """Detect between DAG dependencies for the operator."""
        ...

    @classmethod
    def serialize(
        cls, var: Any, *, strict: bool = ..., use_pydantic_models: bool = ...
    ) -> Any: ...
    @classmethod
    def deserialize(cls, encoded_var: Any, use_pydantic_models: bool = ...) -> Any: ...

class SerializedDAG(DAG, BaseSerialization):
    """
    A JSON serializable representation of DAG.

    A stringified DAG can only be used in the scope of scheduler and webserver, because fields
    that are not serializable, such as functions and customer defined classes, are casted to
    strings.

    Compared with SimpleDAG: SerializedDAG contains all information for webserver.
    Compared with DagPickle: DagPickle contains all information for worker, but some DAGs are
    not pickle-able. SerializedDAG works for all DAGs.
    """

    _decorated_fields = ...
    _CONSTRUCTOR_PARAMS = ...
    _json_schema = ...
    @classmethod
    def serialize_dag(cls, dag: DAG) -> dict:
        """Serialize a DAG into a JSON object."""
        ...

    @classmethod
    def deserialize_dag(cls, encoded_dag: dict[str, Any]) -> SerializedDAG:
        """Deserializes a DAG from a JSON object."""
        ...

    @classmethod
    def to_dict(cls, var: Any) -> dict:
        """Stringifies DAGs and operators contained by var and returns a dict of var."""
        ...

    @classmethod
    def from_dict(cls, serialized_obj: dict) -> SerializedDAG:
        """Deserializes a python dict in to the DAG and operators it contains."""
        ...

class TaskGroupSerialization(BaseSerialization):
    """JSON serializable representation of a task group."""

    @classmethod
    def serialize_task_group(cls, task_group: TaskGroup) -> dict[str, Any] | None:
        """Serialize TaskGroup into a JSON object."""
        ...

    @classmethod
    def deserialize_task_group(
        cls,
        encoded_group: dict[str, Any],
        parent_group: TaskGroup | None,
        task_dict: dict[str, Operator],
        dag: SerializedDAG,
    ) -> TaskGroup:
        """Deserializes a TaskGroup from a JSON object."""
        ...
