"""
This type stub file was generated by pyright.
"""

import enum
from collections.abc import Mapping, Sequence
from typing import TYPE_CHECKING, Any, Literal

from airflow.models import BaseOperator
from airflow.utils.context import Context

if TYPE_CHECKING: ...

class FILE_FORMAT(enum.Enum):
    """Possible file formats."""

    CSV = ...
    JSON = ...
    PARQUET = ...

FileOptions = ...
FILE_OPTIONS_MAP = ...

class SqlToS3Operator(BaseOperator):
    """
    Saves data from a specific SQL query into a file in S3.

    .. seealso::
        For more information on how to use this operator, take a look at the guide:
        :ref:`howto/operator:SqlToS3Operator`

    :param query: the sql query to be executed. If you want to execute a file, place the absolute path of it,
        ending with .sql extension. (templated)
    :param s3_bucket: bucket where the data will be stored. (templated)
    :param s3_key: desired key for the file. It includes the name of the file. (templated)
    :param replace: whether or not to replace the file in S3 if it previously existed
    :param sql_conn_id: reference to a specific database.
    :param sql_hook_params: Extra config params to be passed to the underlying hook.
        Should match the desired hook constructor params.
    :param parameters: (optional) the parameters to render the SQL query with.
    :param aws_conn_id: reference to a specific S3 connection
    :param verify: Whether or not to verify SSL certificates for S3 connection.
        By default SSL certificates are verified.
        You can provide the following values:

        - ``False``: do not validate SSL certificates. SSL will still be used
                (unless use_ssl is False), but SSL certificates will not be verified.
        - ``path/to/cert/bundle.pem``: A filename of the CA cert bundle to uses.
                You can specify this argument if you want to use a different
                CA cert bundle than the one used by botocore.
    :param file_format: the destination file format, only string 'csv', 'json' or 'parquet' is accepted.
    :param max_rows_per_file: (optional) argument to set destination file number of rows limit, if source data
        is larger than that, it will be dispatched into multiple files.
        Will be ignored if ``groupby_kwargs`` argument is specified.
    :param pd_kwargs: arguments to include in DataFrame ``.to_parquet()``, ``.to_json()`` or ``.to_csv()``.
    :param groupby_kwargs: argument to include in DataFrame ``groupby()``.
    """

    template_fields: Sequence[str] = ...
    template_ext: Sequence[str] = ...
    template_fields_renderers = ...
    def __init__(
        self,
        *,
        query: str,
        s3_bucket: str,
        s3_key: str,
        sql_conn_id: str,
        sql_hook_params: dict | None = ...,
        parameters: None | Mapping[str, Any] | list | tuple = ...,
        replace: bool = ...,
        aws_conn_id: str | None = ...,
        verify: bool | str | None = ...,
        file_format: Literal["csv", "json", "parquet"] = ...,
        max_rows_per_file: int = ...,
        pd_kwargs: dict | None = ...,
        groupby_kwargs: dict | None = ...,
        **kwargs,
    ) -> None: ...
    def execute(self, context: Context) -> None: ...
