"""
This type stub file was generated by pyright.
"""

import datetime
from collections.abc import Sized
from typing import TYPE_CHECKING, Any, NamedTuple

from airflow.models import DAG, DagRun
from airflow.utils.trigger_rule import TriggerRule

"""Exceptions used by Airflow."""
if TYPE_CHECKING: ...

class AirflowException(Exception):
    """
    Base class for all Airflow's errors.

    Each custom exception should be derived from this class.
    """

    status_code = ...
    def serialize(self):  # -> tuple[str, tuple[str], dict[Any, Any]]:
        ...

class AirflowBadRequest(AirflowException):
    """Raise when the application or server cannot handle the request."""

    status_code = ...

class AirflowNotFoundException(AirflowException):
    """Raise when the requested object/resource is not available in the system."""

    status_code = ...

class AirflowConfigException(AirflowException):
    """Raise when there is configuration problem."""

    ...

class AirflowSensorTimeout(AirflowException):
    """Raise when there is a timeout on sensor polling."""

    ...

class AirflowRescheduleException(AirflowException):
    """
    Raise when the task should be re-scheduled at a later time.

    :param reschedule_date: The date when the task should be rescheduled
    """

    def __init__(self, reschedule_date) -> None: ...
    def serialize(self):  # -> tuple[str, tuple[()], dict[str, Any]]:
        ...

class InvalidStatsNameException(AirflowException):
    """Raise when name of the stats is invalid."""

    ...

class AirflowTaskTimeout(BaseException):
    """Raise when the task execution times-out."""

    ...

class AirflowTaskTerminated(BaseException):
    """Raise when the task execution is terminated."""

    ...

class AirflowWebServerTimeout(AirflowException):
    """Raise when the web server times out."""

    ...

class AirflowSkipException(AirflowException):
    """Raise when the task should be skipped."""

    ...

class AirflowFailException(AirflowException):
    """Raise when the task should be failed without retrying."""

    ...

class AirflowOptionalProviderFeatureException(AirflowException):
    """Raise by providers when imports are missing for optional provider features."""

    ...

class AirflowInternalRuntimeError(BaseException):
    """
    Airflow Internal runtime error.

    Indicates that something really terrible happens during the Airflow execution.

    :meta private:
    """

    ...

class XComNotFound(AirflowException):
    """Raise when an XCom reference is being resolved against a non-existent XCom."""

    def __init__(self, dag_id: str, task_id: str, key: str) -> None: ...
    def __str__(self) -> str: ...
    def serialize(self):  # -> tuple[str, tuple[()], dict[str, str]]:
        ...

class UnmappableOperator(AirflowException):
    """Raise when an operator is not implemented to be mappable."""

    ...

class XComForMappingNotPushed(AirflowException):
    """Raise when a mapped downstream's dependency fails to push XCom for task mapping."""

    def __str__(self) -> str: ...

class UnmappableXComTypePushed(AirflowException):
    """Raise when an unmappable type is pushed as a mapped downstream's dependency."""

    def __init__(self, value: Any, *values: Any) -> None: ...
    def __str__(self) -> str: ...

class UnmappableXComLengthPushed(AirflowException):
    """Raise when the pushed value is too large to map as a downstream's dependency."""

    def __init__(self, value: Sized, max_length: int) -> None: ...
    def __str__(self) -> str: ...

class AirflowDagCycleException(AirflowException):
    """Raise when there is a cycle in DAG definition."""

    ...

class AirflowDagDuplicatedIdException(AirflowException):
    """Raise when a DAG's ID is already used by another DAG."""

    def __init__(self, dag_id: str, incoming: str, existing: str) -> None: ...
    def __str__(self) -> str: ...

class AirflowDagInconsistent(AirflowException):
    """Raise when a DAG has inconsistent attributes."""

    ...

class AirflowClusterPolicyViolation(AirflowException):
    """Raise when there is a violation of a Cluster Policy in DAG definition."""

    ...

class AirflowClusterPolicySkipDag(AirflowException):
    """Raise when skipping dag is needed in Cluster Policy."""

    ...

class AirflowClusterPolicyError(AirflowException):
    """Raise for a Cluster Policy other than AirflowClusterPolicyViolation or AirflowClusterPolicySkipDag."""

    ...

class AirflowTimetableInvalid(AirflowException):
    """Raise when a DAG has an invalid timetable."""

    ...

class DagNotFound(AirflowNotFoundException):
    """Raise when a DAG is not available in the system."""

    ...

class DagCodeNotFound(AirflowNotFoundException):
    """Raise when a DAG code is not available in the system."""

    ...

class DagRunNotFound(AirflowNotFoundException):
    """Raise when a DAG Run is not available in the system."""

    ...

class DagRunAlreadyExists(AirflowBadRequest):
    """Raise when creating a DAG run for DAG which already has DAG run entry."""

    def __init__(
        self, dag_run: DagRun, execution_date: datetime.datetime, run_id: str
    ) -> None: ...
    def serialize(self):  # -> tuple[str, tuple[()], dict[str, Any]]:
        ...

class DagFileExists(AirflowBadRequest):
    """Raise when a DAG ID is still in DagBag i.e., DAG file is in DAG folder."""

    def __init__(self, *args, **kwargs) -> None: ...

class FailStopDagInvalidTriggerRule(AirflowException):
    """Raise when a dag has 'fail_stop' enabled yet has a non-default trigger rule."""

    _allowed_rules = ...
    @classmethod
    def check(cls, *, dag: DAG | None, trigger_rule: TriggerRule):  # -> None:
        """
        Check that fail_stop dag tasks have allowable trigger rules.

        :meta private:
        """
        ...

    def __str__(self) -> str: ...

class DuplicateTaskIdFound(AirflowException):
    """Raise when a Task with duplicate task_id is defined in the same DAG."""

    ...

class TaskAlreadyInTaskGroup(AirflowException):
    """Raise when a Task cannot be added to a TaskGroup since it already belongs to another TaskGroup."""

    def __init__(
        self, task_id: str, existing_group_id: str | None, new_group_id: str
    ) -> None: ...
    def __str__(self) -> str: ...

class SerializationError(AirflowException):
    """A problem occurred when trying to serialize something."""

    ...

class ParamValidationError(AirflowException):
    """Raise when DAG params is invalid."""

    ...

class TaskNotFound(AirflowNotFoundException):
    """Raise when a Task is not available in the system."""

    ...

class TaskInstanceNotFound(AirflowNotFoundException):
    """Raise when a task instance is not available in the system."""

    ...

class PoolNotFound(AirflowNotFoundException):
    """Raise when a Pool is not available in the system."""

    ...

class NoAvailablePoolSlot(AirflowException):
    """Raise when there is not enough slots in pool."""

    ...

class DagConcurrencyLimitReached(AirflowException):
    """Raise when DAG max_active_tasks limit is reached."""

    ...

class TaskConcurrencyLimitReached(AirflowException):
    """Raise when task max_active_tasks limit is reached."""

    ...

class BackfillUnfinished(AirflowException):
    """
    Raises when not all tasks succeed in backfill.

    :param message: The human-readable description of the exception
    :param ti_status: The information about all task statuses
    """

    def __init__(self, message, ti_status) -> None: ...

class FileSyntaxError(NamedTuple):
    """Information about a single error in a file."""

    line_no: int | None
    message: str
    def __str__(self) -> str: ...

class AirflowFileParseException(AirflowException):
    """
    Raises when connection or variable file can not be parsed.

    :param msg: The human-readable description of the exception
    :param file_path: A processed file that contains errors
    :param parse_errors: File syntax errors
    """

    def __init__(
        self, msg: str, file_path: str, parse_errors: list[FileSyntaxError]
    ) -> None: ...
    def __str__(self) -> str: ...

class ConnectionNotUnique(AirflowException):
    """Raise when multiple values are found for the same connection ID."""

    ...

class TaskDeferred(BaseException):
    """
    Signal an operator moving to deferred state.

    Special exception raised to signal that the operator it was raised from
    wishes to defer until a trigger fires. Triggers can send execution back to task or end the task instance
    directly. If the trigger should end the task instance itself, ``method_name`` does not matter,
    and can be None; otherwise, provide the name of the method that should be used when
    resuming execution in the task.
    """

    def __init__(
        self,
        *,
        trigger,
        method_name: str,
        kwargs: dict[str, Any] | None = ...,
        timeout: datetime.timedelta | None = ...,
    ) -> None: ...
    def serialize(self):  # -> tuple[str, tuple[()], dict[str, Any]]:
        ...
    def __repr__(self) -> str: ...

class TaskDeferralError(AirflowException):
    """Raised when a task failed during deferral for some reason."""

    ...

class RemovedInAirflow3Warning(DeprecationWarning):
    """Issued for usage of deprecated features that will be removed in Airflow3."""

    deprecated_since: str | None = ...

class AirflowProviderDeprecationWarning(DeprecationWarning):
    """Issued for usage of deprecated features of Airflow provider."""

    deprecated_provider_since: str | None = ...

class DeserializingResultError(ValueError):
    """Raised when an error is encountered while a pickling library deserializes a pickle file."""

    def __str__(self) -> str: ...

class UnknownExecutorException(ValueError):
    """Raised when an attempt is made to load an executor which is not configured."""

    ...
